

1. 梯度下降法：
	- 代价函数及其梯度用矩阵表示
	- ![[Pasted image 20230314171722.png]]
	- ![[IMG_20230316_195528_edit_879130107255476.jpg]]
	- 一般需要将x进行特征值放缩，可将xi除以该组数据的最大值，也可将其除以最大值减最小值的差
	- 出现次方项，可用一次方项代替，继续求$\theta$
	- 相关代码：
	```Python
	from numpy import *
	
	# 数据集大小 即20个数据点
	m = 20
	# x的坐标以及对应的矩阵
	X0 = ones((m, 1))  # 生成一个m行1列的向量，也就是x0，全是1
	X1 = arange(1, m+1).reshape(m, 1)  # 生成一个m行1列的向量，也就是x1，从1到m
	X = hstack((X0, X1))  # 按照列堆叠形成数组，其实就是样本数据
	# 对应的y坐标
	Y = array([
	    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
	    11, 13, 13, 16, 17, 18, 17, 19, 21
	]).reshape(m, 1)
	# 学习率
	alpha = 0.01
	
	
	# 定义代价函数
	def cost_function(theta, X, Y):
	    diff = dot(X, theta) - Y  # dot() 数组需要像矩阵那样相乘，就需要用到dot()
	    return (1/(2*m)) * dot(diff.transpose(), diff)
	
	
	# 定义代价函数对应的梯度函数
	def gradient_function(theta, X, Y):
	    diff = dot(X, theta) - Y
	    return (1/m) * dot(X.transpose(), diff)
	
	
	# 梯度下降迭代
	def gradient_descent(X, Y, alpha):
	    theta = array([1, 1]).reshape(2, 1)
	    gradient = gradient_function(theta, X, Y)
	    while not all(abs(gradient) <= 1e-5):
	        theta = theta - alpha * gradient
	        gradient = gradient_function(theta, X, Y)
	    return theta
	
	
	optimal = gradient_descent(X, Y, alpha)
	print('optimal:', optimal)
	print('cost function:', cost_function(optimal, X, Y)[0][0])
	
	
	# 根据数据画出对应的图像
	def plot(X, Y, theta):
	    import matplotlib.pyplot as plt
	    ax = plt.subplot(111)  # 这是我改的
	    ax.scatter(X, Y, s=30, c="red", marker="s")
	    plt.xlabel("X")
	    plt.ylabel("Y")
	    x = arange(0, 21, 0.2)  # x的范围
	    y = theta[0] + theta[1]*x
	    ax.plot(x, y)
	    plt.show()
	
	
	plot(X1, Y, optimal)
	```
2. 正规方程法：
	- ![[Pasted image 20230314172946.png]]
	- 当特征值过多时不适用
	- 当出现$X^TX$不可逆时，可考虑==删去对模型无贡献的特征值或线性相关的特征值==
3. 正则化:
	- 正则化能减小参数的大小，从而使模型不会过拟合
	- 梯度下降法的正则化
		- ![[IMG_20230316_202533_edit_880912981350274.jpg]]
	- 正规方程的正则化
		- ![[Screenshot_20230316_112901_edit_879359907718874.jpg]]
		- 正规方程正则化后可避免$X^TX$不可逆的问题
4. logistic回归:
	- 实现原理依然是梯度下降法，只是修改了代价函数时其限定在0到1内
	- ![[Screenshot_20230315_104527_edit_834942896389919.jpg]]
	- ![[Screenshot_20230315_075857_edit_834953649341843.jpg]]
	- 
5. numpy的矩阵运算:
	- 矩阵乘法：`np.dot(array1,array2)`
	- 数乘：`np.multiply(alpha,array)`  /  `alpha * array`
	- 求逆：`np.linalg.inv(array)`
	- 转置：`array.T` 
	- 创建单位阵E：`np.eye(方阵大小)`
	- 求迹：`np.trace()`
	- 求行列式：`np.linalg.det()`
	- 解方程组：`np.linalg.solve(未知数系数矩阵，常数项系数矩阵)`
	- 获取最大值索引：`np.argmax(data,axis=)`   (axis=1时获取横向方向最大值的列索引，axis=0时获取纵向方向最大值的行索引，不指定axis则为获取整个data最先出现的最大值的索引)
6. softmax回归：
	- 是个多分类问题，相当于一般情况的logistic回归
		- ![[Screenshot_20230330_095404_edit_1380720959317835.jpg]]
		- ![[Screenshot_20230330_095257_edit_1380704232589834.jpg]]
		- ![[Screenshot_20230329_195635_edit_1380690529634217.jpg]]
	- 在进行测试时所用到的矩阵大小及梯度函数：
		- ![[IMG_20230330_154419_edit_1380909929919000 1.jpg]]
		- ![[IMG_20230330_154506_edit_1380970589230080.jpg]]
		- 
7. 遇到的问题：
	- 在学习梯度下降的时候，出现了预测值振荡过大的情况，而且当迭代次数大了之后会出现上溢出的情况。后来我通过改小了学习率才解决了，也让我知道了原来有些梯度下降的学习率会如此之小，比我在视频里看到的常用学习率小了几个数量级
	- 在学习正规方程法时，遇到了矩阵无法相乘的问题，后来我检查了一下，发现是矩阵大小错了，给了我一个教训：在操作向量及矩阵时最好把每个变量的形状列在草稿纸上，这样才不会乱
	- 在学习softmax回归时，遇到的第一个问题是在概率函数的求和分母上，一开始出现的错误是提示我(2000,1)和(2000,)不能进行操作，后来我将错误信息复制到csdn后，才知道原来是求和后是个一维数组，无法进行矩阵广播操作，修改过后果然可以了；第二个问题是遇到了运行极其缓慢的问题，于是我检查了一遍代码，发现梯度函数的式子出现了错误，修正后我又优化了示性函数，使其在一开始便计算完成并生成示性函数矩阵，避免了每一次迭代都要重新计算的问题，全部改正后，运行速度与准确率都大幅上升