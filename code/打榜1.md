

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#使打印的内容全显示
#np.set_printoptions(threshold = 1e6)

#读取数据集
pf = pd.read_csv(r"C:\Users\86189\Desktop\train_.csv")

#处理空值
pf.dropna(inplace=True)
pff = pf.drop(['outcome','id'],axis = 1)

m = len(pf)#m=342

#判断相关性
#for i in range(0,13):
#    print(pf[[str(i),'outcome']].corr())

#向量化x
x0 = np.ones(m).reshape(m,1)
xj = np.array(pff).reshape(m,13)
x = np.hstack((x0,xj))

#生成θ向量，以下θ为迭代一定次数后的，如此生成θ来节省时间
theta = np.array([[ 1.37951168e+00]
 ,[-1.20565819e-01]
 ,[ 6.77314704e-02]
 ,[-6.01090893e-02]
 ,[ 2.49781636e+00]
 ,[ 1.20147163e-01]
 ,[ 5.19517749e+00]
 ,[-4.49108833e-03]
 ,[-1.13399968e+00]
 ,[ 2.56038839e-01]
 ,[-1.07051830e-02]
 ,[-2.64260464e-01]
 ,[ 1.64098605e-02]
 ,[-4.65252149e-01]]
).reshape(14,1)

#生成y向量
y = np.array(pf['outcome']).reshape(m,1)

#生成正则化矩阵
E = np.eye(14)[0]
E[0] = 0


#生成正则化系数lamda
lamda = 0.02
#定义代价函数及其梯度
def J(x,theta,y,m):
    return np.dot((np.dot(x,theta) - y).T,np.dot(x,theta) - y)/(2*m)
def grad_J(x,theta,y,m):
    return np.dot(x.T,np.dot(x,theta) - y)/m
#定义学习率α
alpha = 0.000006373829
step = 600000
for i in range(step):
    theta = theta - np.multiply(alpha,grad_J(x,theta,y,m))
print(theta)
print(J(x,theta,y,m))
    
#开始正则化标准方程计算
#theta = np.dot(np.linalg.inv(np.dot(x.T,x) + lamda * E),np.dot(x.T,y))
#plt.subplot(121)
#plt.plot(np.dot(x,theta))
#plt.show()


#开始测试

pp = pd.read_csv(r"C:\Users\86189\Desktop\test_.csv")
pp.fillna(0,inplace=True)
mp = len(pp)
pp=pp.drop('id',axis=1)
#向量化x
xp0 = np.ones(mp).reshape(mp,1)
xpj = np.array(pp).reshape(mp,13)
xp = np.hstack((xp0,xpj))

outcome = np.dot(xp,theta)

ps = pd.read_csv(r"C:\Users\86189\Desktop\my_submission.csv")
ps['outcome'] = outcome
ps.to_csv(r"C:\Users\86189\Desktop\my_submission2.csv")
```
